{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project: Web Scraping\n",
    "- Name: Levi Grenier\n",
    "- Date: Sep. 29, 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "### Description\n",
    "\n",
    "Both Beautiful Soup and Selenium can do the same things when it comes to webscraping. In this assignment you will use both Beautiful Soup and Selenium to scrape the below website so that you can see how they are similar in functionality as well as how Selenium has a few more advanced options. The below website has many options of movies and TV Shows to pick from. \n",
    "\n",
    "https://subslikescript.com/\n",
    "\n",
    "\n",
    "### Grading\n",
    "\n",
    "For grading purposes, we will clear all outputs from all your cells and then run them all from the top.  Please test your notebook in the same fashion before turning it in.\n",
    "\n",
    "### Submitting Your Solution\n",
    "\n",
    "To submit your notebook, first clear all the cells. Then use the File->Download As->Notebook to obtain the notebook file. You will submit a .zip file with this notebook, the txt from the Beautiful Soup portion, and two more txt's from the Selenium portion.  Finally, submit the .zip file on Canvas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Beautiful Soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests # sends requests to a website\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: Setup Beautiful Soup (5 points)\n",
    "Complete the following tasks:\n",
    "\n",
    "1. Select a movie or TV show on the website that you want to scrape.\n",
    "2. Send a request to the website and then call Beautiful Soup on that request.\n",
    "3. print the result of implementing Beautiful Soup to check your work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "website = 'https://subslikescript.com/movie/Kaguya-sama_Love_Is_War-9816396'\n",
    "result = requests.get(website)\n",
    "context = result.text\n",
    "soup = BeautifulSoup(context, 'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: Website Exploration (10 points)\n",
    "\n",
    "a. Provide code below to produce answers to the following questions (edit this cell with your answers): \n",
    "\n",
    "    1. What is the title of the movie or TV show that you chose?\n",
    "    \n",
    "    Kaguya Sama: Love is War (2019) - full transcript\n",
    "\n",
    "    2. How do you know that this is the title of the article? (think the tag)\n",
    "\n",
    "    We used the h1 tag. This is the first header tag, so it is typically the article's title. \n",
    "\n",
    "b. Use Beautiful Soup to print the text of the title.\n",
    "\n",
    "c. Use Beautiful Soup to print the basic HTML code of the chosen movie or TV show.     \n",
    "\n",
    "d. Use Beautiful Soup to print the text of the full script of the movie or TV show."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a\n",
    "title = soup.find('h1').get_text()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c. \n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d. \n",
    "full_script_text = soup.find('div', class_=\"full-script\").get_text()\n",
    "print(full_script_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3: Create the txt File\n",
    "\n",
    "Now send the entire script of the movie or TV show to a txt file. \n",
    "\n",
    "With the name of the txt file as **Beautiful Soup** with the title of the chosen article. \n",
    "\n",
    "For example: If I chose Titanic the name of the txt file would be:\n",
    "\n",
    "    1. 'Beautiful Soup Titanic (1997) - full transcript.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "title = title.replace(':', \"\",1)\n",
    "with open(f'Beautiful Soup {title}.txt', 'w', encoding=\"utf-8\") as file:\n",
    "    file.write(full_script_text) # can also use the entire_script_text variable they are the same thing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection\n",
    "\n",
    "Now assume at this point that you wanted to look at the script of a different movie. If you were just using Beautiful Soup would it be possible to by **only using code** chose a different movie? How would you have to choose a different movie if Beautiful Soup was your web scraping method of choice?\n",
    "\n",
    "**Discuss new results**\n",
    "> I don't think so. It doesn't even look like there's a suitable URL in the HTML code that I could parse out and then use. I suppose I could not get there only using code and the given webpage. \n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import Select\n",
    "import time\n",
    "import pandas as pd\n",
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4: Setup Selenium\n",
    "We are going to use the **same** movie or TV show as we used above. \n",
    "\n",
    "Complete the following tasks:\n",
    "\n",
    "1. Define the website\n",
    "2. Define the path to the driver.\n",
    "3. Create the driver\n",
    "4. Use the driver to get the website. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "website = 'https://subslikescript.com/movie/Kaguya-sama_Love_Is_War-9816396'\n",
    "path = '/Users/levig/Documents/Mines F22/CSCI 303/chromedriver.exe' \n",
    "driver = webdriver.Chrome(path) \n",
    "driver.get(website)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5: Website Exploration With Selenium\n",
    "\n",
    "**Note that when you use the driver to find_elements it returns a list. You cannot use .text on a list so you have to access the element inside of that resulting list to use .text and print the title.**\n",
    "\n",
    "a. Provide code below to produce answers to the following questions (edit this cell with your answers): \n",
    "\n",
    "    1. What is the title of the movie or TV show that you chose?\n",
    "\n",
    "    KAGUYA-SAMA: LOVE IS WAR (2019) - FULL TRANSCRIPT\n",
    "    \n",
    "    2. What did you use to find_element by? i.e. XPath, ID, TAG_NAME?\n",
    "    \n",
    "    TAG_NAME\n",
    "    \n",
    "    3. If you used XPath, what is the XPath of the title? \n",
    "       If you used ID, what is the ID of the title?\n",
    "       If you used the TAG_NAME, what is the TAG_NAME of the title?\n",
    "       \n",
    "       TAG_NAME: h1\n",
    "\n",
    "b. Use Selenium to print the text of the title. \n",
    "        \n",
    "c. Use Selenium to print the text of the main article of the chosen movie or TV show.\n",
    "\n",
    "    1. What did you use to find_element by? i.e. XPath, ID, TAG_NAME?\n",
    "    \n",
    "    I used XPATH.\n",
    "    \n",
    "    2. If you used XPath, what is the XPath? \n",
    "       If you used ID, what is the ID?\n",
    "       If you used the TAG_NAME, what is the TAG_NAME?\n",
    "\n",
    "    XPATH: '//article[@class=\\'main-article\\']'\n",
    "    \n",
    "    It took me a bit to realize that I needed the escape character before the other quotation marks.\n",
    "\n",
    "d. Use Selenium to print the text of the full script of the movie or TV show.\n",
    "\n",
    "    1. What did you use to find_element by? i.e. XPath, ID, TAG_NAME?\n",
    "    \n",
    "    2. If you used XPath, what is the XPath? \n",
    "       If you used ID, what is the ID?\n",
    "       If you used the TAG_NAME, what is the TAG_NAME?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a.\n",
    "title2 = driver.find_element(By.TAG_NAME, 'h1').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b.\n",
    "print(title2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c.\n",
    "article2 = driver.find_element(By.XPATH, '//article[@class=\\'main-article\\']') # I know it's the first time I'm using \"article\", but I just want it to be consistent.\n",
    "print(article2.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d.\n",
    "full_script_text2 = driver.find_element(By.XPATH, '//div[@class=\\'full-script\\']').text\n",
    "print(full_script_text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 6: Create the txt File\n",
    "\n",
    "Now send the entire script of the movie or TV show to a txt file. \n",
    "\n",
    "With the name of the txt file as **Selenium** with the title of the chosen article. \n",
    "\n",
    "For example: If I chose Titanic the name of the txt file would be:\n",
    "\n",
    "    1. 'Selenium Titanic (1997) - full transcript.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title2 = title2.replace(':',\"\")\n",
    "with open(f'Selenium {title2}.txt', 'w', encoding=\"utf-8\") as file:\n",
    "    file.write(full_script_text2) # can also use the entire_script_text variable they are the same thing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection\n",
    "\n",
    "At this point you have done the exact same thing with Selenium that you have done with Beautiful Soup. Take a few minutes and reflect on the following questions: \n",
    "\n",
    "    1. At this stage in the assignment has Selenium or Beautiful Soup been easier to use? Why has this method been easier to use?\n",
    "    2. If you were to implement webscraping in the future would you prefer to use Beautiful Soup or Selenium?\n",
    "    3. What aspects of Beautiful Soup did you prefer over Selenium? What aspects of Selenium did you prefer over Beautiful Soup?\n",
    "\n",
    "\n",
    "**Put your answers here**(Edit this cell)\n",
    "> 1. It has not been easier to use. It took me a lot longer to understand the XPATH, ID, and TAG_NAME stuff. \n",
    "> 2. Probably Beautiful Soup, but it dpends on the context. Selenium is powerful.\n",
    "> 3. I liked how unpolished it was and how it didn't try to do things for me. I like knowing exactly what's happening to my data. With Selenium, I'm not sure how it is handling formatting and other things. Fewer variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 7: Choosing a Different Movie/TV Show\n",
    "\n",
    "By now I am sure you are tired of the same movie!\n",
    "\n",
    "I'm curious on if any movies start with the letter 'X'.\n",
    "\n",
    "Can you help me out and look at movies that start with the letter X?\n",
    "\n",
    "**hint: you will have to push 3 different buttons for this**\n",
    "\n",
    "**ALSO THIS PAGE HAS POP UP ADS. THE WEBSITE FOLLOWS ALONG IN LIVE TIME AS YOU CLICK ON THESE LINKS SO IF A POP UP AD APPEARS JUST CLICK 'CLOSE' ON THE POP UP AD AND KEEP SELENIUM'ING AWAY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# click on the movie link to go back\n",
    "movie_button = driver.find_element(By.XPATH, '//a[@href=\\'/movies\\']')\n",
    "movie_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# click on the 'X' link to look at movies that start with the letter X\n",
    "x_button = driver.find_element(By.XPATH, '//a[@href=\\'/movies_letter-X\\']')\n",
    "x_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now click on your movie\n",
    "x_men_button = driver.find_element(By.XPATH, '/html/body/div/div/main/article/ul/a[15]') #I found an option in Inspect that said \"copy XPATH\". Looks like it works!\n",
    "x_men_button.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 8: Create the txt File and Supporting Information\n",
    "\n",
    "Now that the new movie/TV show is loaded up we are almost done!\n",
    "\n",
    "Do these next 3 things:\n",
    "\n",
    "    1. Get the title of the movie/TV show.\n",
    "    \n",
    "    2. Get the full-script of the movie/TV show.\n",
    "    \n",
    "    3. Create a txt file of the full-script with the naming scheme for the txt as 'Selenium2' and then the title then .txt. So say I choose Titanic the name of my txt to submit would be: 'Selenium2 Titanic (1997) - full transcript.txt'.\n",
    "    \n",
    "    4. Lastly, remember to tell the driver to quit as we are done scraping!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the title and then put it into text form\n",
    "title3 = driver.find_element(By.TAG_NAME, 'h1').text\n",
    "print(title3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the full-script and then put it into text form\n",
    "full_script_text3 = driver.find_element(By.XPATH, '//div[@class=\\'full-script\\']').text\n",
    "print(full_script_text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the txt file\n",
    "title3 = title3.replace(':',\"\")\n",
    "with open(f'Selenium2 {title3}.txt', 'w', encoding=\"utf-8\") as file:\n",
    "    file.write(full_script_text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are done scraping so quit the driver.\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Data Visualization With Web Scraping\n",
    "\n",
    "Now what we want to do is utilize our web scraping skills to implement data manipulation techniques that we have learned earlier in this course. \n",
    "\n",
    "Your task for this last part is to load in that **soccer_data.csv** file that we created at the end of the **Selenium WebScraper** lesson and run any data manipulation techniques on that data that you would like. You can put it into a DataFrame, run a classification model on it, show visulazations, etc. Anything that we have learned in this course up to this point is free game. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 9: Data Manipulation on Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the csv (make sure the csv is in the same directory as this project)\n",
    "soccer_data = pd.read_csv('soccer_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do your desired data manipulation technique!\n",
    "import numpy as np\n",
    "\n",
    "score_data = soccer_data['score']\n",
    "home_score = []\n",
    "away_score = []\n",
    "total_score = []\n",
    "team_wins = dict()\n",
    "team_goals = dict()\n",
    "#for date, home, score, away in soccer_data:\n",
    "for i in range(len(soccer_data)):\n",
    "    # Scores\n",
    "    scores = soccer_data.iloc[i,2].split(\" - \")\n",
    "    home_score.append(int(scores[0]))\n",
    "    away_score.append(int(scores[1]))\n",
    "    total_score.append(int(scores[0])+int(scores[1]))\n",
    "    \n",
    "    # Winner count\n",
    "    winner = \"\"\n",
    "    home = soccer_data.iloc[i,1]\n",
    "    away = soccer_data.iloc[i,3]\n",
    "    if int(scores[0]) > int(scores[1]):\n",
    "        winner = home\n",
    "    elif int(scores[0]) < int(scores[1]):\n",
    "        winner = away\n",
    "    else:\n",
    "        continue    \n",
    "    if team_wins.get(winner) is None:\n",
    "        team_wins[winner] = 1\n",
    "    else:\n",
    "        team_wins[winner] += 1\n",
    "    \n",
    "    # Team goals\n",
    "    if team_goals.get(home) is None:\n",
    "        team_goals[home] = int(scores[0])\n",
    "    else:\n",
    "        team_goals[home] += int(scores[0])\n",
    "    if team_goals.get(away) is None:\n",
    "        team_goals[away] = int(scores[1])\n",
    "    else:\n",
    "        team_goals[away] += int(scores[1])\n",
    "        \n",
    "# Scatterplot of score combinations\n",
    "plt.scatter(home_score, away_score)\n",
    "plt.title(\"Score Combinations\")\n",
    "plt.xlabel(\"Home Score\")\n",
    "plt.ylabel(\"Away Score\")\n",
    "plt.show()\n",
    "\n",
    "# Histogram of average scores\n",
    "plt.hist(total_score, bins = max(total_score))\n",
    "plt.title(\"Score Distribution\")\n",
    "plt.xlabel(\"Total Score\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "print(f\"Average score: {sum(total_score)/len(total_score)}\")\n",
    "\n",
    "# Scatter plot of gaols and wins\n",
    "wins = []\n",
    "goals = []\n",
    "for k,v in team_wins.items():\n",
    "    wins.append(v)\n",
    "    goals.append(team_goals[k])\n",
    "\n",
    "# Relationship between total goals and total wins\n",
    "plt.scatter(wins, goals)\n",
    "plt.title(\"Total Goals vs Wins for Individual Teams\")\n",
    "plt.xlabel(\"Total Wins\")\n",
    "plt.ylabel(\"Total Goals\")\n",
    "plt.show()\n",
    "correlation = np.corrcoef(wins, goals)\n",
    "print(f\"Correlation between total goals and total wins: {correlation[0][1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection\n",
    "\n",
    "At this point you have now done some sort of data manipulation with data that you scrapped online! This is important because most data that you will work with will not be given to you. You have to go out and collect the data in order to work with it which is exactly what we did here. Answer the following reflection questions regarding Part 3:\n",
    "   \n",
    "   1. What data manipulation technique did you do on the data?\n",
    "   2. What did the results of your data manipulation tell you about the data? Any hidden meanings or things of value that you found in the data?\n",
    "   \n",
    "**Put your answers here**(Edit this cell)\n",
    "    \n",
    ">    I made visuals showing the score combinations, the score distribution, and the total score vs total wins for each of the teams. \n",
    "\n",
    "    \n",
    ">    It told me that total score and total wins are highly correlated. It told be that the average number of goals in a game was 2.82. It also showed me a \"limiting distance\", so to speak, in the score combinations plot. \n",
    "    However, this analysis didn't tell me much -- it was very simple. I would like to do a cluster analysis including the difference between the winner and loser's scores and the date as features. The question this would be looking to answer is if there are teams that have similar patterns of dominance over time.\n",
    "    (Sorry I didn't play with this more. I need to catch a flight tomorrow morning.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You Finished! Treat yourself by taking this questionnaire\n",
    "### Questionnaire\n",
    "1) How long did you spend on this assignment?\n",
    "<br>Like three hours.<br>\n",
    "2) What did you like about it? What did you not like about it?\n",
    "<br>I'm in a rush, so that's tainting my perception of it. that being said, I liked the concept quite a lot. I specially liked how you left a sort of free-response exploratory question at the end. I would like to see that in other projects (but it might be more effective to place that question as it's own mini-assignment as having it at the end of a difficult assignment like this will probably make people explore less).<br>\n",
    "3) Did you find any errors or is there anything you would like changed?\n",
    "<br>You said that there wasn't data on the website for 22/23 season in the Selenium tutorial, but there is now. <br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
