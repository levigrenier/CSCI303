{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xQOMtFFs6LkH"
   },
   "source": [
    "# Project 3 : NumPy, matplotlib, and Python functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Po55TcPV6LkK"
   },
   "source": [
    "## Instructions\n",
    "\n",
    "### Description\n",
    "\n",
    "Below is a guided exercise in doing linear regression using a gradient descent algorithm.  The goal here isn't to teach you the gradient descent algorithm, or scientific computing in general, but to give you experience with using NumPy for matrix-vector math, matplotlib for simple visualizations, and more Python (specifically, writing functions).\n",
    "\n",
    "### Grading\n",
    "\n",
    "For grading purposes, we will clear all outputs from all your cells and then run them all from the top.  Please test your notebook in the same fashion before turning it in.\n",
    "\n",
    "### Submitting Your Solution\n",
    "\n",
    "To submit your notebook, first clear all the cells (this won't matter too much this time, but for larger data sets in the future, it will make the file smaller).  Then use the File->Download As->Notebook to obtain the notebook file.  Finally, submit the notebook file on Canvas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_hXQYBES6LkK"
   },
   "source": [
    "## Setup Code\n",
    "\n",
    "Feel free to look through this and see what it is doing, but basically it is just setting up a synthetic regression problem for you, very much like we did in class.  Just make sure you execute the cell before going on to the problems!  If you wish to see the training and testing points, uncomment the last two lines in the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zjG7LR3t6LkL"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def f(X):\n",
    "    return 1/(1 + np.exp(-X))\n",
    "\n",
    "n = 50\n",
    "np.random.seed(12345)\n",
    "trainX = np.random.random(n) * 10 - 5\n",
    "trainY = f(trainX) + np.random.randn(n) * 0.125\n",
    "testX = np.random.random(n) * 10 - 5\n",
    "testY = f(testX) + np.random.randn(n) * 0.125\n",
    "#plt.plot(trainX, trainY, 'b.', testX, testY, 'gx')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "hmOKyOm26LkL"
   },
   "source": [
    "#### Problem 1: Features (10 points)\n",
    "For this problem, you need to create a Python function named `phi` that will produce a set of between 5 and 8 features for a given X input, such as the trainX vector created above.  This is basically what we did in lectures 5-7 to generate our design matrix $\\Phi$.  The difference is that in our lectures we used powers of $x$ exclusively, where as below I would like you to use at least two non-polynomial functions of $x$.  You can use any functions you want; consider trigonometric functions, logarithmic and exponential functions, radial basis function (Gaussians), etc.  The only functions you are forbidden to use are functions generating a sigmoid such as the one generating the synthetic data for this problem (this includes the arctangent function)!  It is recommended, but not required, that one of your features is an intercept term (all 1's).\n",
    "\n",
    "Note the triple-quoted string at the top of the function - this produces a documentation comment for the function (visible when you do help(function) or function?).  You should get in the habit of making these for your code.\n",
    "\n",
    "Note: We discussed a variation of this in the `06-ml-beginnings lecture` ... your code will just have to create a list containing different functions of x to send into the np.array() instead of using all powers of x like we did for trainX in the example: `Phi = np.array([trainX ** p for p in range(6)]).T`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y6lZwxCw6LkM"
   },
   "outputs": [],
   "source": [
    "def phi(X):\n",
    "    '''\n",
    "    Return a design matrix (a NumPy ndarray) whose columns are functions of the input vector, X.\n",
    "    '''\n",
    "    Phi = np.array([X**0, X**1, X**2, X**3, np.cos(X), np.exp(X), np.sin(X)]).T\n",
    "    \n",
    "    return Phi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oWIsputr6LkM"
   },
   "source": [
    "You should probably check the output of phi(trainX) below, just to make sure none of your functions generated NaNs (Not a Number values) or infs (infinities) - these can result, e.g, from taking the log of a negative number or division by a number close to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5XBSsMIX6LkM"
   },
   "outputs": [],
   "source": [
    "#calls your newly created phi function above for testing\n",
    "Phi = phi(trainX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2SvmqHyM6LkN"
   },
   "source": [
    "#### Problem 2: Gradient Descent (20 points)\n",
    "The basic idea behind [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent) is that you can minimize a parametrized function by simply taking small steps in the negative direction of the derivative of the function.  I'm not going to go into too many details here, since [Wikipedia's article](https://en.wikipedia.org/wiki/Gradient_descent) does such a good job.  Also see [Andrew Ng's class notes](https://datajobs.com/data-science-repo/Generalized-Linear-Models-[Andrew-Ng].pdf) on the topic for a more thorough mathematical treatment.\n",
    "\n",
    "In our cases, we are interesting in minimizing the square error in our linear approximation with respect to our training data.  That is, we want to minimize\n",
    "$$ \\begin{align}\n",
    "     & \\sum_{i=0}^n (\\hat{f}(x_i) - y_i)^2 \\\\\n",
    "   = & \\sum_{i=0}^n (\\phi_0(x_i) w_0 + \\phi_1(x_i) w_1 + \\dots + \\phi_k(x_i) w_k - y_i)^2 \\\\ \n",
    "   = & \\|\\Phi w - \\textbf{y}\\|^2 \n",
    "   \\end{align}\n",
    "$$\n",
    "\n",
    "Since all of our training data (the $x_i$ and $y_i$ terms) are known, the only variables we can optimize on are the $w$ variables. The gradient is (skipping over a *lot* of linear algebra):\n",
    "\n",
    "$$ \\begin{align}\n",
    "       & \\nabla \\|\\Phi w - \\textbf{y}\\|^2  \\\\\n",
    "     = & \\Phi^T (\\Phi w - \\textbf{y})\n",
    "   \\end{align}\n",
    "$$\n",
    "\n",
    "The gradient descent algorithm asks us, then, to start with some guess for $\\textbf{w}$ and repeatedly subtract some small multiple of the gradient from it, until $\\textbf{w}$ converges:\n",
    "\n",
    "$$ \\textbf{w} = \\textbf{w} + \\alpha \\Phi^T (\\textbf{y} - \\Phi \\textbf{w}) $$\n",
    "\n",
    "Convergence is a somewhat tricky issue, and the step size $\\alpha$ is a key to getting good convergence; a large $\\alpha$ speeds up the algorithm, but can miss the optimum by stepping too far (and can even diverge); a small $\\alpha$ may result in a better optimum, but at the cost of speed.  You may have to play around a bit with different values for $\\alpha$ until you get a good result; print out the norm of the change in `w` at each step to see if you are converging or diverging, or put in an `if` statement to kill the function when the norm starts growing large.  In testing this assignment, I found I had to use a pretty small $\\alpha$ to prevent divergence; 0.00001 worked for me.  However, you will get different results depending on your features.\n",
    "\n",
    "So, you will write a function `lr` which takes in $\\Phi$ (generated by your `phi` function above applied to `trainX`) and $y$ (`trainY`) and returns a vector `w` computed by gradient descent.  Follow the steps below to compute w:\n",
    "\n",
    " 1. Start with `w` as the zero vector.\n",
    " 2. Initialize `norm`, `delta`, and step counter.\n",
    " 3. Calculate `delta` (The amount `w` is going to change calculated as: $\\alpha \\Phi^T (\\textbf{y} - \\Phi \\textbf{w})$ )\n",
    " 4. Update `w` and step counter\n",
    " 5. Calculate `norm` (the norm of a vector is the square root of the dot product of the vector with itself)\n",
    " 6. Repeat steps 3-5 until counter is greateer than 100,000 or `norm` is less than 1e-6. \n",
    "\n",
    "Before returning, your function should print out the number of iterations (gradient descent steps) taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X9ILmig26LkO"
   },
   "outputs": [],
   "source": [
    "def lr(Phi, y):\n",
    "    '''\n",
    "    Computes a linear regression result w minimizing ||Phi w - y||^2 via gradient descent.\n",
    "    '''    \n",
    "    # Initializing variables\n",
    "    w = np.array([0] * Phi.shape[1])\n",
    "    norm = 1\n",
    "    delta = 0.0001*Phi.T@(y-Phi@w)\n",
    "    counter = 0\n",
    "    lastNorm = 2\n",
    "    \n",
    "    # Gradient descent loop\n",
    "    while norm >= 1e-6 and counter <= 100000:\n",
    "        counter += 1 \n",
    "        delta = 0.00001*Phi.T@(y-Phi@w)\n",
    "        w = w + delta\n",
    "        norm = np.sqrt(delta.T @ delta)\n",
    "        if norm - lastNorm > 0.01:\n",
    "            print(f\"Steps taken: {counter}\")\n",
    "            break\n",
    "        lastNorm = norm\n",
    "        \n",
    "    print(f\"Steps taken: {counter}\")\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lZUgl2ZL6LkO"
   },
   "source": [
    "#### Problem 3: Compute RMSE (10 points)\n",
    "Use your `lr` function above to compute a solution, then using your solution ($\\hat f(x)$), compute the RMSE of your result on **both the training and the test data sets** , and print them below. (For reference, I was able to achieve 0.11574 with 8 features.) Remeber that MSE is:\n",
    "\n",
    " $$ MSE = \\frac{1}{n} \\sum_{i = 1}^n (y_i - \\hat f(x_i))^2 $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bQr7zvxD6LkO"
   },
   "outputs": [],
   "source": [
    "# Call the linear regression\n",
    "w = lr(Phi, trainY)\n",
    "\n",
    "# Calculate the RMSE\n",
    "def rmse(Phi, w, y):\n",
    "    diff = y - (Phi @ w)\n",
    "    RMSE = np.sqrt(sum(diff**2)/n)\n",
    "    return RMSE\n",
    "\n",
    "# Print the results\n",
    "print(\"RMSE Training: \", rmse(Phi, w, trainY))\n",
    "print(\"RMSE Test: \", rmse(Phi, w, testY))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c5QAYTmb6LkO"
   },
   "source": [
    "#### Problem 4: Plot Results (10 points)\n",
    "You are going to create three plots below. On the first plot, show the training and testing points (using markers of different colors), and a line plot of your learned function (compute $\\hat{y}$ for $x \\in (-5,5)$ at increments of 0.1).  This can be accomplished using `plt.plot(...)`, following the examples in the lectures.  Don't forget you can also look at the matplotlib documentation - there's a link under the Help menu in your notebook.  If you feel especially daring, you can try doing the plot in [bokeh](http://bokeh.pydata.org/) instead.\n",
    "\n",
    "Your second and third plots should be of the residuals on the training set.  (The residuals are the errors, or differences between the training $y$ values and the corresponding $\\hat{y}$ values.)  There are various ways to look at the residuals; for the second plot, use `plt.scatter(...)` (or bokeh equivalent) to plot the training $x$ values versus the residuals for the training $y$ values.  If the values are roughly equally distributed above and below the $x$ axis, then our approximation was probably pretty good.\n",
    "\n",
    "The third plot should use `plt.hist(...)` (or bokeh equivalent) to show a histogram of the residuals (use 5 bins or so).  Given the small number of samples, it is unlikely you'll see a very normal distribution, but it should be vaguely normal, assuming we nearly approximated $f(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uLoQmcYS6LkP"
   },
   "outputs": [],
   "source": [
    "# Calculate the Predicted Values and create a Function for Later\n",
    "def f(x,w):\n",
    "    return w[0] + w[1]*x + w[2]*x**2 + w[3]*x**3 + w[4]*np.cos(x) + w[5]*np.exp(x) + w[6]*np.sin(x)\n",
    "X = np.arange(-5,5,0.1)\n",
    "phi_hat = phi(X)\n",
    "y_hat = phi_hat @ w\n",
    "\n",
    "# Line Plot\n",
    "labels = [\"Training Data\", \"Test Data\", \"Linear Regression\"]\n",
    "plt.plot(trainX, trainY, 'b.', testX, testY, 'gx', X, y_hat, 'r-')\n",
    "plt.legend(labels)\n",
    "plt.title(\"Linear Regression of Training and Test Data\")\n",
    "plt.ylabel(\"Y Value\")\n",
    "plt.xlabel(\"X Value\")\n",
    "plt.show()\n",
    "\n",
    "# Residual Plot for Training Data\n",
    "residuals = trainY-(phi(trainX)@w)    \n",
    "plt.scatter(trainX, residuals, label=\"Residuals\")\n",
    "plt.axhline(y=0, color = 'k', linestyle = \":\")\n",
    "plt.legend()\n",
    "plt.title(\"Residuals Plot: Training Values minus Predicted Values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.xlabel(\"X Value\")\n",
    "plt.show()\n",
    "\n",
    "# Histogram of Residuals\n",
    "plt.hist(residuals, bins = 7)\n",
    "plt.title(\"Histogram of Residuals\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xlabel(\"Residual\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "03-fitting (1).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
