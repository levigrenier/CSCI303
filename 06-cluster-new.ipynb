{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 6 : Clustering\n",
    "- Name: Levi Grenier\n",
    "- Date: Sep. 29, 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "### Description\n",
    "\n",
    "Practice clustering on a using the well known and very popular `Iris` Dataset! The Iris flower data set is fun for learning supervised classification algorithms, and is known as a difficult case for unsupervised learning. \n",
    "https://cran.r-project.org/web/packages/dendextend/vignettes/Cluster_Analysis.html\n",
    "<br><br>Yes, there are many examples out there, but see if you can do it yourself :). We can easily hypothesize on how many clusters would yield the best result, so let us prove it through a simple experiment that you could repeat with additional data sets.\n",
    "\n",
    "### Grading\n",
    "\n",
    "For grading purposes, we will clear all outputs from all your cells and then run them all from the top.  Please test your notebook in the same fashion before turning it in.\n",
    "\n",
    "### Submitting Your Solution\n",
    "\n",
    "To submit your notebook, first clear all the cells (this won't matter too much this time, but for larger data sets in the future, it will make the file smaller).  Then use the File->Download As->Notebook to obtain the notebook file.  Finally, submit the notebook file on Canvas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import datasets\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: Data Generation (5 points)\n",
    "Reference for more information: Chapter 5.11 K-Means in the online course book.\n",
    "\n",
    "1. Load the `iris` dataset and separate into `X` and `y` variables (our ground truth labels will just be used for visualization).\n",
    "2. Write a hypothesis on how many clusters will yield the best labeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hypothesis**\n",
    "> I hypothesize that 6 clusters will yield the best labeling.\n",
    ">\n",
    "> (Because the data exploration doesn't happen until after the hypothesis, I have not used knowledge of the data set to inform my hypothesis. Just curious why this would take place before looking at the data.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: Data exploration (10 points)\n",
    "\n",
    "This is the step where you would normally conduct any needed preprocessing, data wrangling, and investigation of the data.\n",
    "<br>**Note:** `print(iris.DESCR)` prints the iris dataset description, provided you loaded it into a variable named `iris`\n",
    "\n",
    "a. Using your skills from previous projects, provide code below to produce answers to the following questions (edit this cell with your answers): \n",
    "\n",
    "    1. How many features are provided?\n",
    "\n",
    "    There are four features provided (not including the target feature). \n",
    "\n",
    "    2. How many total observations?\n",
    "    \n",
    "    There are 150 total observations in this dataset.\n",
    "\n",
    "    3. How many different labels are included, what are they called, and is it a balanced dataset with the same number of observations for each class?\n",
    "    \n",
    "    There are four different labels in X. They are 'sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', and 'petal width (cm)'. There are fifty observations in each class. It is a balanced dataset with one-third of the data falling under each of the three classes.\n",
    "    \n",
    "        \n",
    "b. Create a 2D or 3D scatter plot of two or three of the features and use the y labels for color coding. Do not reduce the data or number of features in any way (you will do this by applying PCA in problem 5).\n",
    "\n",
    "c. Since clusters can be influenced by the magnitudes of the variables, scale the feature data and plot a histogram of the transformed feature data (think about if you should use the min-max, standard scaler, or normalizer).\n",
    "\n",
    "I have looked at each column's histogram for standard, MinMax, and nomalized scaling. Normalized has largely coherent and well-grouped histograms, so I am concerned that we will not be able to distringuish specific clusters. MinMax na dstandard scaling have similar histograms. The difference between the two that has lead me to my choice is that for  the histogram of petal width, we see that MinMax divides a set of observations into two distinct groups whereas standard scaling does not divide that group (the second one in the historgram). Thus, I have chosen to use MinMax scaling.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a\n",
    "print(iris.DESCR)\n",
    "print(iris.feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b\n",
    "\n",
    "# 2D scatterplot for sepal length and width\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.plot(X[0:49,0], X[0:49,1], 'r.', X[50:99,0], X[50:99,1], 'b.', X[100:149,0], X[100:149,1], 'y.',)\n",
    "plt.title(\"Sepal Length vs. Sepal Width\")\n",
    "plt.xlabel(\"Sepal Length (cm)\")\n",
    "plt.ylabel(\"Sepal Width (cm)\")\n",
    "plt.show()\n",
    "\n",
    "# 3D scatter plot for sepal width, petal length, and petal width \n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(X[0:49,1], X[0:49,2], X[0:49,3], color = 'red')\n",
    "ax.scatter(X[50:99,1], X[50:99,2], X[50:99,3], color = 'blue')\n",
    "ax.scatter(X[100:149,1], X[100:149,2], X[100:149,3], color = 'yellow')\n",
    "ax.set_xlabel(\"Sepal Width\")\n",
    "ax.set_ylabel(\"Petal Length\")\n",
    "ax.set_zlabel(\"Petal Width\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c. Scale the data (think about if you should use the min-max, standard scaler, or normalizer)\n",
    "\n",
    "col = 3\n",
    "\n",
    "# Standard Scale\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "ss = StandardScaler()\n",
    "ss.fit(X)\n",
    "X_scaled = ss.transform(X)\n",
    "plt.hist(X_scaled[0:49,col])\n",
    "plt.title(\"X_scaled histogram\")\n",
    "plt.show()\n",
    "\n",
    "# Normalized\n",
    "from sklearn.preprocessing import Normalizer\n",
    "normalized = Normalizer()\n",
    "normalized.fit(X)\n",
    "X_norm = normalized.transform(X)\n",
    "plt.hist(X_norm[0:49,col])\n",
    "plt.title(\"X_norm histogram\")\n",
    "plt.show()\n",
    "\n",
    "# MinMax\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X)\n",
    "X_minmax = scaler.transform(X)\n",
    "plt.hist(X_minmax[0:49,col])\n",
    "plt.title(\"X_minmax histogram\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3: Unsupervised Learning - Clustering (15 points)\n",
    "Conduct clustering experiments with one of algorithms discussed in class (e.g., k-means) for number of clusters k = 2-10. Create another 2D or 3D scatter plot utilizing the <b>cluster assignments</b> for color coding (this output can be a plot for each of the values of k or just one final plot using the value of k from your best Silhouette result obtained in Problem 4 below).  \n",
    "\n",
    "#### Steps:\n",
    "Repeat for each value of k (maybe a loop here would be appropriate):\n",
    "1. Create model object\n",
    "2. Train or fit the model\n",
    "3. Predict cluster assignments\n",
    "4. Calculate Silhouette width (see Problem 4)\n",
    "4. Plot points color coded by class labels predicted by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "silhouette_vals = []\n",
    "\n",
    "for k in range(2, 10):  \n",
    "    # Declaring and fitting the model\n",
    "    km = KMeans(k)\n",
    "    km.fit(X_minmax)\n",
    "    prediction = km.predict(X_minmax)\n",
    "    \n",
    "    # Plotting the results\n",
    "    plt.scatter(X[:,0], X[:,1], c=prediction)\n",
    "    plt.show()\n",
    "    \n",
    "    # Silhouette Score\n",
    "    silhouette_avg = silhouette_score(X_minmax, prediction)\n",
    "    print(f\"For k = {k} clusters, the average silhouette_score is: {silhouette_avg}\")\n",
    "    silhouette_vals.append(silhouette_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4: Evaluate results (20 points)\n",
    "\n",
    "As we have discussed, validating an usupervised problem is difficult. There is a metric that can be used to determine the density or separation of cluster assignments, called Silhouette width. In this step, perform analysis of results using the above `k = 2-10` and compute the Silhouette width (Hint: possibly you can just add code to your loop in problem 3 and store the results in a list of values). \n",
    "\n",
    "Scikit Learn has a great example for Silhouette analysis [here](http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html)\n",
    "\n",
    "1. For each k (k = 2-10), what are the Silhouette width values?\n",
    " \n",
    " See printed statements in Problem 3. See plot below. \n",
    " \n",
    "\n",
    "2. Discuss if your best number of clusters (highest Silhouette width value) matches your hypothesis from Problem 1.\n",
    "\n",
    " For k=2, we have the highest silhouette value at 0.63. This disproves my hypothesis in which k=6 was the ideal number of clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(2,10), silhouette_vals, 'o-')\n",
    "plt.title(\"Silhouette Values for k-many clusters\")\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"Silhouette Value\")\n",
    "\n",
    "print(f\"We see that at k = 2, we have the highest silhouette value at {max(silhouette_vals)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5 (15 points): Principal Component Analysis (PCA)\n",
    "PCA is the most popular form of dimensionality reduction, which basically, rotates and transforms the data into a new subspace, such that the resultant matrix has:\n",
    "- Most relevance (variation) now associated with first feature\n",
    "- Second feature gets the next most, etc.\n",
    "#### Steps:\n",
    "    1. Reduce the feature data (X) using PCA\n",
    "    2. Repeat the same experiment from problem 3 above (remember your plots are now the 1st, 2nd, and possibly 3rd principal component vs. the raw feature data like before).\n",
    "    3. Compare and contrast results to those from previous/non-PCA problems; does it perform better/worse/same? Provide discussion below (this could vary, depending on setup)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering with PCA\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)\n",
    "data_pca = pca.transform(X)\n",
    "\n",
    "silhouette_vals_pca = []\n",
    "\n",
    "for k in range(2, 10):  \n",
    "    # Declaring and fitting the model\n",
    "    km = KMeans(k)\n",
    "    km.fit(data_pca)\n",
    "    prediction = km.predict(data_pca)\n",
    "    \n",
    "    # Plotting the results\n",
    "    plt.scatter(X[:,0], X[:,1], c=prediction)\n",
    "    plt.show()\n",
    "    \n",
    "    # Silhouette Score\n",
    "    silhouette_avg = silhouette_score(data_pca, prediction)\n",
    "    print(f\"For k = {k} clusters, the average silhouette_score is: {silhouette_avg}\")\n",
    "    silhouette_vals_pca.append(silhouette_avg)\n",
    "\n",
    "print(f\"We see that at k = 2, we have the highest silhouette value at {max(silhouette_vals_pca)}\")\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discuss new results**\n",
    "> We can see that the silhouette value for two clusters has improved by 0.075! That's pretty good considering that we just transformed it using the first two principal components. We should also note that both approaches predicted two clusters as ideal, so we can be pretty confident in that regard. \n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You Finished! Treat yourself by taking this questionnaire\n",
    "### Questionnaire\n",
    "1) How long did you spend on this assignment?\n",
    "<br> Like three and a half hours.<br>\n",
    "2) What did you like about it? What did you not like about it?\n",
    "<br> I liked this a lot. I feel like it is super interesting (though I am concerned that it may seem unmotivated to my peers). I especially liked that we explored the silhouette analysis. I really like having something that I can use to make a model decision. <br>\n",
    "3) Did you find any errors or is there anything you would like changed?\n",
    "<br>Did not find anything of note.<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
