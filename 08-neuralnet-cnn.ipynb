{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O1HWTVel3pBY"
   },
   "source": [
    "# Project 8: CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dE3SLyWQ3pBb"
   },
   "source": [
    "## Instructions\n",
    "\n",
    "### Description\n",
    "\n",
    "In this project you will be learning about Convolutional Neural Networks (CNN) and messing with the parameters to see how they can change the output of the CNN. You will start with some questions about Tensorflow, the python package used for CNN! Good luck!\n",
    "\n",
    "### Grading\n",
    "\n",
    "For grading purposes, we will clear all outputs from all your cells and then run them all from the top.  Please test your notebook in the same fashion before turning it in.\n",
    "\n",
    "### Submitting Your Solution\n",
    "\n",
    "To submit your notebook, first clear all the cells (this won't matter too much this time, but for larger data sets in the future, it will make the file smaller).  Then use the File->Download As->Notebook to obtain the notebook file.  Finally, submit the notebook file on Canvas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mUWr69gS3pBb"
   },
   "source": [
    "\n",
    "## 40 points\n",
    "\n",
    "### Instructions\n",
    "- The **\"MNIST\" data set** is composed of 28x28 pixel, black and white images of handwritten digits (0-9). It is commonly used to demonstrate the training and testing of CNNs.\n",
    "- **We provide you with framework code** to build a CNN of roughly suitable size for the MNIST digit recognition task.\n",
    "\n",
    "\n",
    "- **Your tasks**:\n",
    "    1. Read the linked article and answer the questions under the \"What is Tensorflow\" section\n",
    "    2. **Execute the train/test code** and observe the results of the gradient descent training.\n",
    "    3. Complete all 4 modifications of the code, thee modification instructions are:\n",
    "         - **Modify the model architecture in the following ways**, and repeat the training and testing. Each modification should be made relative to the original network. Do not keep \"adding\" each modification with each new model (which would lead to a final model having all the modifications below):\n",
    "           - **Modification 1**: Remove the ReLU activation functions and the max-pooling layers, thereby making the entire network a linear function.\n",
    "           - **Modification 2**: Increase the size of the model by a factor of 16, roughly.\n",
    "           - **Modification 3**: Convert the large convolutional model to a large non-convolutional model, with roughly the same number of parameters.\n",
    "           - **Modification 4**: Train the original baseline model with a larger training data set. \n",
    "     4. MAKE SURE THAT: along the way, **answer the questions in the Jupyter notebook cells**. You can just do this with comments within the cell, for example...\n",
    "   - ```## Answer 1: The training loss increased when I did x, y, and z.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QCYQem1i3pBc"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import time\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3DhCDGsy3pBc"
   },
   "source": [
    "# Part 1\n",
    "## What is TensorFlow? (6 pts)\n",
    "\n",
    "Before we get started with the rest of the project, let's talk about what tensorflow is. You are required to read this article https://www.guru99.com/what-is-tensorflow.html, and answer the following questions:\n",
    "\n",
    "1. What are the three parts of the TensorFlow Architecture?\n",
    "\n",
    "    The three parts of the TensorFlow archiecture (as given in the article) are (1) preprocessing the data, (2) building the model, and (3) training/using the model. \n",
    "\n",
    "\n",
    "2. What is a Tensor and how does it represent data?\n",
    "\n",
    "    A tensor is like a matrix but with more than two axes. It represents data in the same way that a matrix or vector would: though storing some observation in a location indexed by a vector of length n (where n is the dimension/rank of this tensor). \n",
    "\n",
    "\n",
    "3. What is one advantage of the use of graphs in TensorFlow?\n",
    "\n",
    "    Using graphs fits very nicely with the use of tensors as it allows all operations to be done by \"connecting tensors together.\" Additionally, it is computationally convenient as it can be run on multiple CPUs and it is more portable than other methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R1Fo9v0U3pBd"
   },
   "source": [
    "# Part 2\n",
    "## Set TensorFlow verbosity level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WoHe-pP33pBd"
   },
   "outputs": [],
   "source": [
    "verbose = 2 # 0==no output, 1=accuracy/loss output, 2=progress bar output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1RXp_avK3pBd"
   },
   "source": [
    "## Create helper function to plot results of our model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ofeKzDa03pBe"
   },
   "outputs": [],
   "source": [
    "def plot_results(history):\n",
    "    epoch_num = np.arange(1, len(history.history['loss'])+1)\n",
    "\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epoch_num, history.history['loss'], label='training_loss')\n",
    "    plt.plot(epoch_num, history.history['val_loss'], label='test_loss')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epoch_num, history.history['accuracy'], label='training_accuracy')\n",
    "    plt.plot(epoch_num, history.history['val_accuracy'], label='test_accuracy')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6iBWOpYg3pBe"
   },
   "source": [
    "## Load the MNIST data\n",
    "\n",
    "### You will not need to re-run or re-use (cut/paste) the three code cells below.\n",
    "\n",
    "The full dataset has 60,000 training images and 10,000 test images.\n",
    "\n",
    "To accelerate training on jupyterhub (with lots of users) we will only  \n",
    "use a subset of the training set. This will also let us explore some of the  \n",
    "hazards of training a neural net without a very large set of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 546,
     "status": "ok",
     "timestamp": 1623263362505,
     "user": {
      "displayName": "Christopher Johnson",
      "photoUrl": "",
      "userId": "06055017626944874848"
     },
     "user_tz": 360
    },
    "id": "ykQxHK9o3pBe",
    "outputId": "0f04cc0d-5811-4752-e2f4-e4f25a8605da"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "(images_train, labels_train), (images_test, labels_test) = mnist.load_data()\n",
    "\n",
    "# Use a subset of the full training and test sets for actual training and testing,\n",
    "# to accelerate training, and demonstrate possible pitfalls of smaller training data sets.\n",
    "\n",
    "n_train = 1000\n",
    "images_train = images_train[0:n_train,:,:]\n",
    "labels_train = labels_train[0:n_train]\n",
    "\n",
    "n_test = 1000\n",
    "images_test = images_test[0:n_test,:,:]\n",
    "labels_test = labels_test[0:n_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IclCPaMl3pBf"
   },
   "source": [
    "## Let's graph a few of the MNIST digits, to confirm that they look as expected\n",
    "\n",
    "Using plt.imshow(), graph the first sixteen numbers in the training images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264
    },
    "executionInfo": {
     "elapsed": 1906,
     "status": "ok",
     "timestamp": 1623263673305,
     "user": {
      "displayName": "Christopher Johnson",
      "photoUrl": "",
      "userId": "06055017626944874848"
     },
     "user_tz": 360
    },
    "id": "FT8COBMl3pBf",
    "outputId": "e1ad2a86-2eaa-4096-a244-eebcf7fe4145"
   },
   "outputs": [],
   "source": [
    "## You will not need to run this cell more than once, or cut/paste it elsewhere\n",
    "plt.figure(figsize=(8*2, 2*2))\n",
    "for i in range(16):\n",
    "    plt.subplot(2,8,i+1)\n",
    "    plt.imshow(images_train[i,:,:], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UfeGjXdl3pBf"
   },
   "outputs": [],
   "source": [
    "# Create TensorFlow Dataset objects to hold train and test data.\n",
    "images_train = images_train/255 \n",
    "images_train = np.expand_dims(images_train, axis=3) # TensorFlow expects a channel dimension\n",
    "images_train = tf.cast(images_train, tf.float32)\n",
    "labels_train = tf.cast(labels_train, tf.float32)\n",
    "dataset_train = tf.data.Dataset.from_tensor_slices((images_train, labels_train))\n",
    "\n",
    "images_test = images_test/255\n",
    "images_test = np.expand_dims(images_test, axis=3) # TensorFlow expects a channel dimension\n",
    "images_test = tf.cast(images_test, tf.float32)\n",
    "labels_test = tf.cast(labels_test, tf.float32)\n",
    "dataset_test = tf.data.Dataset.from_tensor_slices((images_test, labels_test))\n",
    "\n",
    "batch_size = 50\n",
    "\n",
    "dataset_train = dataset_train.cache()\n",
    "dataset_train = dataset_train.shuffle(n_train)\n",
    "dataset_train = dataset_train.batch(batch_size)\n",
    "dataset_train = dataset_train.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "dataset_test = dataset_test.cache()\n",
    "dataset_test = dataset_test.batch(batch_size)\n",
    "dataset_test = dataset_test.cache()\n",
    "dataset_test = dataset_test.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JnOhRh5m3pBg"
   },
   "source": [
    "## Construct, compile, and train the baseline model. (4 pts)\n",
    "\n",
    "\n",
    "The code will plot the loss and accuracy scores that were collected during training. Remember that training uses gradient descent, so the model parameters are slowly updated as the model gets a closer and closer fit to the data. TensorFlow records the scores after each \"epoch\"--the number of iterations at which all samples in the training set have been used once, in the gradient descent process.\n",
    "\n",
    "We are now going to build teh model. In tensorflow, we build the model by first defining all the layers that the model is going to have, then we compile the model. Finally, we can train the model. We are going to define the model first. The rest is taken care of for you. \n",
    "\n",
    "We do this by calling `tf.keras.models.Sequential()` and inputting the list of layers, in order, as the parameter. Our baseline model is going to include two convalutional layers with pooling layers after each one, then we are going to have one dense layer with 64 neurons. It will then conclude with a dense layer of 10 that is our output layer. (Hint, you need to reduce the dimensionality before the data enters the neural network part of the model!)\n",
    "\n",
    "The following are names of layers that may be useful: \n",
    "\n",
    "  `tf.keras.layers.Conv2D()`\n",
    "\n",
    "  `tf.keras.layers.MaxPool2D()`\n",
    "\n",
    "  `tf.keras.layers.Flatten()`\n",
    "\n",
    "  `tf.keras.layers.Dense()`\n",
    "\n",
    "For our model, we want to make sure that our convolutional layers have a kernel size of 3x3, 4 kernels, and use the 'relu' activation. Our pooling layers are going to have a 2x2 pooling same and `padding='same'`.   Don't foget to add the relu activation to the dense layer with 64 neurons. \n",
    "\n",
    "More information about the layers can be found [here](https://www.tensorflow.org/api_docs/python/tf/keras/layers)!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LmtrItii3pBg",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## This is the baseline model. \n",
    "\n",
    "##Your Code goe here\n",
    "num_kernels = 4\n",
    "dense_layer_neurons = 64\n",
    "kernels_size = (3,3)\n",
    "pooling_size = (2,2)\n",
    "model = tf.keras.models.Sequential([tf.keras.layers.Conv2D(num_kernels, kernels_size, activation = \"relu\"),\n",
    "    tf.keras.layers.MaxPool2D(pooling_size, padding = \"same\"),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(num_kernels, kernels_size, activation = \"relu\"),\n",
    "    tf.keras.layers.MaxPool2D(pooling_size, padding = \"same\"),\n",
    "    \n",
    "    tf.keras.layers.Flatten(),\n",
    "    \n",
    "    tf.keras.layers.Dense(dense_layer_neurons, activation = \"relu\"),\n",
    "    tf.keras.layers.Dense(10)]\n",
    ")\n",
    "\n",
    "# Do not change any arguments in the call to model.compile()\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),    \n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "# Do not change any arguments in the call to model.fit()\n",
    "epochs = 30\n",
    "t = time.time()\n",
    "history = model.fit(dataset_train,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=dataset_test,\n",
    "                    verbose=verbose)\n",
    "print('Training duration: %f seconds.' % (time.time() - t))\n",
    "\n",
    "# Plot results\n",
    "plot_results(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OQBQXyMS3pBg"
   },
   "source": [
    "## Part 2 Questions (6 pts)\n",
    "\n",
    "### Using the baseline model, answer the questions below"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "k5Vf-v453pBh"
   },
   "source": [
    "##Assume one image is pushed through as the sole input, and answer with three dimensions.\n",
    "\n",
    "##(Hint: The default stride for MaxPool2D is 2, and the default stride for Conv2D is 1!)\n",
    "\n",
    "##Questions 1: What is the dimensionality of the input before it has gone through any layers?\n",
    "##Question 2: What is its dimensionality after passing through the first convulational layer?\n",
    "##Question 3: What is its dimensionality after passing through the first convulational layer and the first pooling layer?\n",
    "\n",
    "##Answer 1: \n",
    "## (28,28,1), representing each pixel in the image.\n",
    "##Answer 2: \n",
    "## (26,26,4), because the default of padding is 'valid', we are using a kernel size of (3,3), and we're using num_kernels = 4 (this gives the 4 in the final dimension). \n",
    "##Answer 3:\n",
    "## (13,13,4), because we have a 2x2 pooling window that moves 2 steps between uses -- thus reducing our 26x26 data to 13x13."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TQcKuuXa3pBh"
   },
   "source": [
    "## MODIFICATION 1 (6 pts)\n",
    "\n",
    "### Copy code from cell above, that builds and trains the baseline model, and plots the results.\n",
    "### Now alter the model as described below, and run the code.\n",
    "Alter the model by removing the ReLU and max-pooling non-linear activations.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IQz6T56a3pBi",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## MODIFICATION 1\n",
    "## BELOW, PUT YOUR MODEL CONSTRUCTION, COMPILATION, AND FITTING CODE\n",
    "\n",
    "#%tensorboard --logdir logs\n",
    "# I have no clue what this is/I don't think this was ever explained. I've just commented it \n",
    "# out because it looks like it isn't supposed to be here (as none of the other modification sections have it).\n",
    "\n",
    "num_kernels = 4\n",
    "dense_layer_neurons = 64\n",
    "kernels_size = (3,3)\n",
    "pooling_size = (2,2)\n",
    "model = tf.keras.models.Sequential([tf.keras.layers.Conv2D(num_kernels, kernels_size),    \n",
    "    tf.keras.layers.Conv2D(num_kernels, kernels_size),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(dense_layer_neurons),\n",
    "    tf.keras.layers.Dense(10)]\n",
    ")\n",
    "\n",
    "# Do not change any arguments in the call to model.compile()\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),    \n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "# Do not change any arguments in the call to model.fit()\n",
    "epochs = 30\n",
    "t = time.time()\n",
    "history = model.fit(dataset_train,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=dataset_test,\n",
    "                    verbose=verbose)\n",
    "print('Training duration: %f seconds.' % (time.time() - t))\n",
    "\n",
    "# Plot results\n",
    "plot_results(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4dkGl--g3pBi"
   },
   "outputs": [],
   "source": [
    "## MODIFICATION 1\n",
    "## QUESTION 1: During training, how did the loss (error) curves change from the baseline model to the linear (MOD 1)\n",
    "##             model, for both the training and test sets? What does this imply regarding underfitting or overfitting?\n",
    "## QUESTION 2: In general, did the ReLU and max-pooling non-linearities make for a better model or a worse model?\n",
    "\n",
    "## Answer 1:\n",
    "## In the training set, the loss error curve had a normal decline. In the test set, the loss error curve started \n",
    "## going up after a quick dip. This suggests that our model is overfitting the data. \n",
    "## \n",
    "## Answer 2:\n",
    "## I think they made for a better model as the results for the test and training sets were more comparable than the test \n",
    "## and training results our modified model gives. \n",
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7KkqETXL3pBi"
   },
   "source": [
    "## MODIFICATION 2 (6 pts)\n",
    "\n",
    "### Copy code from the baseline model, for building, training, and plotting results.\n",
    "### Now alter the model as described below, and run the code.\n",
    "\n",
    "Make the network much larger, by:\n",
    "1. Increasing the number of kernels in the convolutional layers from 4 to 64 (16x).\n",
    "2. Increasing the number neurons in the dense layer from 64 to 1024 (16x)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N8Gv4vnO3pBj",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## MODIFICATION 2\n",
    "## BELOW, PUT YOUR MODEL CONSTRUCTION, COMPILATION, AND FITTING CODE\n",
    "num_kernels = 16\n",
    "dense_layer_neurons = 1024\n",
    "kernels_size = (3,3)\n",
    "pooling_size = (2,2)\n",
    "model = tf.keras.models.Sequential([tf.keras.layers.Conv2D(num_kernels, kernels_size, activation = \"relu\"),\n",
    "    tf.keras.layers.MaxPool2D(pooling_size, padding = \"same\"),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(num_kernels, kernels_size, activation = \"relu\"),\n",
    "    tf.keras.layers.MaxPool2D(pooling_size, padding = \"same\"),\n",
    "    \n",
    "    tf.keras.layers.Flatten(),\n",
    "    \n",
    "    tf.keras.layers.Dense(dense_layer_neurons, activation = \"relu\"),\n",
    "    tf.keras.layers.Dense(10)]\n",
    ")\n",
    "\n",
    "# Do not change any arguments in the call to model.compile()\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),    \n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "# Do not change any arguments in the call to model.fit()\n",
    "epochs = 30\n",
    "t = time.time()\n",
    "history = model.fit(dataset_train,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=dataset_test,\n",
    "                    verbose=verbose)\n",
    "print('Training duration: %f seconds.' % (time.time() - t))\n",
    "\n",
    "# Plot results\n",
    "plot_results(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5PK2re6g3pBj"
   },
   "outputs": [],
   "source": [
    "## MODIFICATION 2\n",
    "## QUESTION 1: How did the performance of the larger model (MOD 2) compare to that of the baseline model?\n",
    "## QUESTION 2: Based on the training curves, does the larger model show any **clear** signs of overfitting,\n",
    "##             despite the large number of parameters?\n",
    "\n",
    "## Answer 1:\n",
    "## This one is much better. Lower loss and higher accuracy for training and test sets. I don't see a downside. \n",
    "##\n",
    "## Answer 2:\n",
    "## Not really. We can see a little bit of an uplift in the loss curve for the test set, but it isn't dramatic/clearly \n",
    "## identifiable yet. We have no clear signs of overfitting.\n",
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DTglwvK03pBj"
   },
   "source": [
    "## MODIFICATION 3 (6 pts)\n",
    "\n",
    "### Copy code from the baseline model, for building, training, and plotting results.\n",
    "### Now alter the model as described below, and run the code.\n",
    "\n",
    "**Relative to the MOD 2 model**, remove the convolutional and max-pooling layers, replacing them with a single, new, dense layer.\n",
    "\n",
    "The two convolutional layers have 640 and 36,928 parameters, respectively, for a total of 37,586.\n",
    "For an input of 28x28 = 784 pixels (features), a dense layer with 48 neurons will have roughly 784x48 = 37,632 parameters. Thus, do the following:\n",
    "1. Remove the convolutional and max-pooling layers.\n",
    "2. After the Flatten() layer, add a new Dense layer with 48 neurons and a ReLU activation function.\n",
    "3. Keep the (now second) Dense layer of dense_layer_neurons==1024 neurons, and the final Dense layer of 10 neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2phSJLJ73pBj",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## MODIFICATION 3\n",
    "## BELOW, PUT YOUR MODEL CONSTRUCTION, COMPILATION, AND FITTING CODE\n",
    "num_kernels = 4\n",
    "dense_layer_neurons = 64\n",
    "kernels_size = (3,3)\n",
    "pooling_size = (2,2)\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(48, activation = \"relu\"),\n",
    "    tf.keras.layers.Dense(1024, activation = \"relu\"),\n",
    "    tf.keras.layers.Dense(10)]\n",
    ")\n",
    "\n",
    "# Do not change any arguments in the call to model.compile()\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),    \n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "# Do not change any arguments in the call to model.fit()\n",
    "epochs = 30\n",
    "t = time.time()\n",
    "history = model.fit(dataset_train,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=dataset_test,\n",
    "                    verbose=verbose)\n",
    "print('Training duration: %f seconds.' % (time.time() - t))\n",
    "\n",
    "# Plot results\n",
    "plot_results(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qq4KE1tL3pBk"
   },
   "outputs": [],
   "source": [
    "## MODIFICATION 3\n",
    "## QUESTION 1: How did the performance of the large non-convolutional model (MOD 3) compare to that of the\n",
    "##             large convolutional model (MOD 2)?\n",
    "## QUESTION 2: Speculate on why the convolutional model performs better or worse than the non-convolutional model.\n",
    "##             There is a \"right\" answer, but we're just looking for your opinion/guess. No penalty for error.\n",
    "\n",
    "## Answer 1:\n",
    "## This one is worse. We see a diverging loss curve for the test set (implying overfitting) and our test accuracy is lower \n",
    "## than it was in MOD 2. \n",
    "## Answer 2:\n",
    "## I expect that this is because our convolutional model includes operations that account for neighboring elements. I think \n",
    "## knowing how different a pixel is from its neighboring pixels is useful for me when I'm identifying a digit. It seems like\n",
    "## the convolution model captures something more close to how we really see things.\n",
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lhBUhsR03pBk"
   },
   "source": [
    "## MODIFICATION 4 (6 pts)\n",
    "\n",
    "## This model/data with take longer to train the the previous ones--upwards of several minutes.\n",
    "\n",
    "### For the modification, we don't modify the model, we modify the size of the data sets. The two cells below create training and testing sets with 10,000 samples each, whereas our previous models used only 1,000 samples each.\n",
    "\n",
    "### Copy code from the baseline model, for building, training, and plotting results.\n",
    "- Place the copied code after the two cells below.  \n",
    "- Run all the cells below to get the new data sets, and train the baseline model on that data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eHFU460w3pBk"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "(images_train, labels_train), (images_test, labels_test) = mnist.load_data()\n",
    "\n",
    "# Use a subset of the full training and test sets for actual training and testing,\n",
    "# to accelerate training, and demonstrate possible pitfalls of smaller training data sets.\n",
    "\n",
    "n_train = 10000\n",
    "images_train = images_train[0:n_train,:,:]\n",
    "labels_train = labels_train[0:n_train]\n",
    "\n",
    "n_test = 10000\n",
    "images_test = images_test[0:n_test,:,:]\n",
    "labels_test = labels_test[0:n_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-5barLrn3pBk"
   },
   "outputs": [],
   "source": [
    "# Create TensorFlow Dataset objects to hold train and test data.\n",
    "images_train = images_train/255\n",
    "images_train = np.expand_dims(images_train, axis=3) # TensorFlow expects a channel dimension\n",
    "images_train = tf.cast(images_train, tf.float32)\n",
    "labels_train = tf.cast(labels_train, tf.float32)\n",
    "dataset_train = tf.data.Dataset.from_tensor_slices((images_train, labels_train))\n",
    "\n",
    "images_test = images_test/255\n",
    "images_test = np.expand_dims(images_test, axis=3) # TensorFlow expects a channel dimension\n",
    "images_test = tf.cast(images_test, tf.float32)\n",
    "labels_test = tf.cast(labels_test, tf.float32)\n",
    "dataset_test = tf.data.Dataset.from_tensor_slices((images_test, labels_test))\n",
    "\n",
    "dataset_train = dataset_train.cache()\n",
    "dataset_train = dataset_train.shuffle(n_train)\n",
    "dataset_train = dataset_train.batch(batch_size)\n",
    "dataset_train = dataset_train.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "dataset_test = dataset_test.cache()\n",
    "dataset_test = dataset_test.batch(batch_size)\n",
    "dataset_test = dataset_test.cache()\n",
    "dataset_test = dataset_test.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "czOw9yfw3pBk"
   },
   "outputs": [],
   "source": [
    "## MODIFICATION 4\n",
    "## BELOW, PUT YOUR MODEL CONSTRUCTION, COMPILATION, AND FITTING CODE\n",
    "num_kernels = 4\n",
    "dense_layer_neurons = 64\n",
    "kernels_size = (3,3)\n",
    "pooling_size = (2,2)\n",
    "model = tf.keras.models.Sequential([tf.keras.layers.Conv2D(num_kernels, kernels_size, activation = \"relu\"),\n",
    "    tf.keras.layers.MaxPool2D(pooling_size, padding = \"same\"),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(num_kernels, kernels_size, activation = \"relu\"),\n",
    "    tf.keras.layers.MaxPool2D(pooling_size, padding = \"same\"),\n",
    "    \n",
    "    tf.keras.layers.Flatten(),\n",
    "    \n",
    "    tf.keras.layers.Dense(dense_layer_neurons, activation = \"relu\"),\n",
    "    tf.keras.layers.Dense(10)]\n",
    ")\n",
    "\n",
    "# Do not change any arguments in the call to model.compile()\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "# Do not change any arguments in the call to model.fit()\n",
    "epochs = 30\n",
    "t = time.time()\n",
    "history = model.fit(dataset_train,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=dataset_test,\n",
    "                    verbose=verbose)\n",
    "print('Training duration: %f seconds.' % (time.time() - t))\n",
    "\n",
    "# Plot results\n",
    "plot_results(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ulZPMS_j3pBl"
   },
   "outputs": [],
   "source": [
    "## MODIFICATION 4\n",
    "## QUESTION 1: How did the performance of the baseline model trained on the larger data set (MOD 4) compare\n",
    "##             to that trained on the smaller data set?\n",
    "## QUESTION 2: This is just guess on your part... how much better do you think results might be if you trained\n",
    "##             the model on all 60,000 training samples (rather than 1000 or 10,000)?\n",
    "\n",
    "## Answer 1:\n",
    "## This one is one of the best yet. We have very good accuracies and our loss curves get very small. The only downside is \n",
    "## that our test loss curve looks like it is diverging a little. This is a warning sign of overfitting in our model. \n",
    "##\n",
    "## Answer 2:\n",
    "## If we trained on all of the samples, I think our model would probably get less accurate when identifying a new digit\n",
    "## as it would overfit to the MNIST digits we are using.\n",
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qkAtboF-3pBm"
   },
   "source": [
    "### Questionnaire\n",
    "1) How long did you spend on this assignment?\n",
    "<br> I spent three hours or so on this assignment.<br>\n",
    "2) What did you like about it? What did you not like about it?\n",
    "<br> No comment. I did not feel particularly strongly about any aspect of it.<br>\n",
    "3) Did you find any errors or is there anything you would like changed?\n",
    "<br>What is \"%tensorboard --logdir logs\"? Why was it in the given code? <br>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "08-neuralnet-cnn.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
